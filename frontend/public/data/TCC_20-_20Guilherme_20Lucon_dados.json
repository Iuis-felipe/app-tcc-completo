{
    "Autor": "Guilherme Lucon Pinto",
    "Título": "UM ESTUDO COMPARATIVO ENTRE BANCO DE DADOS RELACIONAL EM DISCO E EM MEMÓRIA",
    "Ano de publicação": 2016,
    "Local de publicação": "Araranguá",
    "Orientador(a)": "Prof. Dr. Alexandre Leopoldo Gonçalves",
    "Coorientador(a)": null,
    "Resumo": "No atual cenário da Tecnologia da Informação vem se produzindo e consumindo cada vez mais dados. Estima-se que entre 2010 e 2020 o volume de dados deverá aumentar 40% ao ano. Estes dados são produzidos em alta velocidade e necessitam de tratamento em tempo hábil. Em decorrência do volume e da velocidade, os dados ultrapassam a capacidade de processamento dos bancos de dados convencionais. Apesar de nas últimas décadas a capacidade de processamento ter crescido conforme a projeção da lei de Moore, o desempenho dos sistemas de armazenamento não foi capaz de acompanhar tal evolução. Para aumentar a capacidade de obter e analisar grandes volumes de dados é necessário utilizar métodos de análise adequados que suportem o incremento no volume de dados. Este trabalho possui como objetivo realizar um estudo comparativo, com base em aspectos gerais de desempenho, entre bancos de dados relacionais em disco e em memória. Para tal, utilizou-se um sistema para realizar a carga dos dados nos bancos de dados escolhidos, bem como, foram realizadas consultas para analisar o desempenho em operações de leitura e escrita nos bancos de dados escolhidos. Com base nos resultados obtidos, observa-se que quanto maior a base de dados, maior o impacto causado pelas baixas velocidades de leitura e escrita em disco rígido. A partir disso, acredita-se que as aplicações responsáveis por armazenar e analisar dados serão baseadas em memória principal.",
    "Palavras-chave": "Banco de Dados Relacional, Banco de Dados em Memória, Sistemas de Armazenamento.",
    "Introdução": "O contexto atual da Tecnologia da Informação e a sua inserção nos mais variados setores tem promovido uma revolução na forma como as pessoas consomem e produzem dados. Os dados produzidos são provenientes das mais diversas fontes de dados (redes sociais, banco de dados, intranet, smartphones) e possuem diferentes formatos, podendo ser estruturados ou não estruturados. Pesquisa realizada em 2014 pela empresa de consultoria em tecnologia, a IDC, aponta que o universo digital deverá crescer 40% ao ano na próxima década, gerando um volume expressivo de dados (IDC, 2014). Também segundo a IDC este aumento no universo digital representaria em 2020 cerca de 40 zetabytes de dados. Tal acréscimo significativo advém principalmente do aumento no uso das tecnologias da informação e comunicação (TICs) pelos diferentes setores da sociedade (GROBELNIK, 2012). Neste contexto, surge o termo Big Data, com o objetivo de caracterizar o aumento expressivo no volume de informação. Apesar de não existir uma definição clara, vários autores têm contribuído para tal. Dumbill (2012), afirma que o termo é utilizado para descrever extensos volumes de dados que possuem diferentes formatos e que são manipulados em alta velocidade. O’Reilly (2012) enfatiza que o termo Big Data representa grandes volumes de dados que excedem a capacidade de processamento dos sistema convencionais de banco de dados. Para Gartner (2014), extensos volumes de dados, alta velocidade, e dados de diferentes fontes e formatos exigem novas formas de processamento. Os autores Fan e Bifet (2012) salientam que o conceito de Big Data vem recebendo atenção dos pesquisadores, tanto por parte da academia quanto da indústria, e, representa desafios devido à natureza dos dados ser evolutiva e volumosa. Em virtude desta natureza é necessário incluir novas maneiras advindas da tecnologia para analisar, tratar e processar os dados (O'REILLY, 2012). Isso ocorre não somente por conta do volume e velocidade de produção, mas devido aos formatos e esquemas distintos dos dados (FAN; BIFET, 2012; O'REILLY, 2012). Para a empresa SAS ®1 o termo Big Data descreve o imenso volume de dados estruturados e não estruturados que impactam nos negócios e no dia a dia de uma organização. Contudo, não somente em casos envolvendo o mundo dos negócios o Big Data está inserido. O conceito de grande volume de dados e a evolução da Internet das Coisas 1 www.sas.com 24 (IoT, do inglês Internet of Things) passam a desempenhar um papel cada vez mais relevante na viabilidade de diversas iniciativas voltadas, por exemplo, para cidades e ambientes inteligentes. Isto se torna possível a partir da integração de sensores sem fio em ambientes reais utilizando redes de serviços. A combinação do IoT e Big Data representa desafios, mas também oportunidades na oferta de serviços que agreguem valor às organizações e aos usuários. Os desafios se concentram principalmente sobre problemas relacionados a negócios e tecnologias referentes a um ambiente inteligente (HASHEM et al., 2016). Pode-se citar como outros exemplos o Colisor de Hádrons (LHC, do inglês Large Hadron Collider) que anualmente é capaz de gerar uma quantidade próxima a 30 petabytes de dados (CERN, 2016), ou o radiotelescópio ASKAP (Australian Square Kilometer Array Pathfinder, em inglês) que transmite a quantidade de 2.8 Gigabytes de dados por segundo (IDC, 2014). Ainda segundo a empresa IBM ®2, 2.5 quintilhões de bytes de dados são criados todos os dias através dos mais variados meios, como por exemplo, em sensores usados para obter informações climáticas, entre outros. Apesar do poder computacional dos processadores ter crescido nas últimas décadas, conforme a perspectiva da lei de Moore, o desempenho dos sistemas de armazenamento não foi capaz de acompanhar tal evolução (JANUKOWICZ, EASTWOOD, 2012). Com base nessa discrepância e nos pontos principais do conceito de Big Data, acredita-se que as aplicações que lidam com esta perspectiva necessitam de uma infraestrutura capaz de ser escalável e paralela (MUKHERJEE et al., 2012). Hashem et al. (2016) ressaltam que o modelo de programação para o processamento de grandes conjuntos de dados com algoritmos paralelos pode ser usado para análise de dados e para obtenção de valor sobre os dados armazenados. Tendo como objetivo aumentar a capacidade de obter e analisar grandes volumes de dados, é necessário o uso de métodos de análise adequados, sendo a técnica de clustering (agrupamento) um dos mais usados (BAHRILL; TIWARI; MALVIYA, 2016). Além dos métodos de clustering, também existem frameworks para lidar com o contexto atual de grandes volumes de dados, entre eles, o Apache Hadoop® (MUKHERJEE et al., 2012), e o Apache Spark® (BAHRILL; TIWARI; MALVIYA, 2016). Porém, a busca de informações em meio a grandes quantidades de dados é difícil de ser realizada nos Sistemas de Gerenciamento de Banco de Dados (SGBD) atuais devido às entradas e saídas de dados no disco rígido dos servidores e à demora no acesso entre o dispositivo de armazenamento e o servidor de aplicação (PLATTNER, ZEIER, 2011). Com o objetivo de facilitar a busca de informações considerando extensos volumes de dados surge a tecnologia in-memory, permitindo carregar e executar todos os dados na memória, reduzindo o tempo de recuperação por determinada informação. De maneira geral, o investimento nesta tecnologia vem se tornando viável com a queda do custo das memórias de computador e passa a ser uma alternativa capaz de lidar adequadamente com o crescente volume dos dados.",
    "Conclusão": "O objetivo geral deste trabalho foi realizar um estudo comparativo entre as abordagens de Banco de Dados Relacional Tradicional (em disco) e o modelo Relacional em Memória, considerando aspectos gerais de desempenho no que tange a população e consulta de dados. A partir do levantamento bibliográfico na área de Banco de Dados, buscou-se entender o domínio e o contexto do trabalho de forma que a partir deste conhecimento fosse possível implementar uma aplicação capaz de realizar inserções dos dados para atender o objetivo proposto. Este estudo foi aplicado sobre a base de dados do serviço de recomendação de filmes MovieLens®. Tal base é composta por informações contidas na base de Base de Dados de Filmes na Internet (IMDb). Com o objetivo de simular um cenário real, para que a comparação dos bancos pudesse ser realizada, foi efetuada uma avaliação de desempenho. Foram executados estudos com o maior conjunto de dados possível, cerca de 25 milhões de linhas, divididos em três partes, uma com todos os dados, outra com cerca de 13 milhões de linhas e a última com cerca de 7 milhões de linhas. Considerando a implementação da aplicação de carga foi possível analisar a inserção e coleta de resultados a partir do estabelecimento de parâmetros comparativos sobre o desempenho apresentado nos dois conceitos de armazenamentos de dados escolhidos. O modelo baseado no conceito de armazenamento em disco apresentou um desempenho similar em comparação ao modelo baseado em memória quando a carga de dados contida no banco não tinha um tamanho expressivo. No entanto, deve-se salientar que essa similaridade de resultados com um volume menor de dados ocorreu devido ao método utilizado para popular o modelo em memória. A carga foi realizada às estruturas já presentes em disco para a memória, o que fez com que a população do banco em memória fosse limitado pela velocidade do disco. O modelo baseado no conceito de armazenamento em memória apresentou uma melhora considerável em todas as três etapas do processo avaliativo, em decorrência do modelo não sofrer com o gargalo gerado pela baixa velocidade de leitura e escrita de um disco, por manter toda a base de dados na memória principal. Mesmo mantendo toda a base em memória, este modelo consegue manter as propriedades ACID, pois permite salvar periodicamente em disco logs com o estado do banco, para que no caso de alguma falha mais grave seja possível a recuperação do banco ao estado anterior consistente. De modo geral, pode-se concluir que o modelo orientado a memória apresenta desempenho similar ao modelo tradicional quando a carga de dados é pequena. Entretanto, à medida que aumenta a quantidade de entradas na base, o banco em memória começa a se distanciar do modelo tradicional, devido às limitações do disco e, por conseguir manter os dados no mesmo local (memória) onde são realizadas todas as operações, tanto de escrita quanto de leitura. Este fator diminui consideravelmente o tempo de busca de dados para execução de consultas. Durante o desenvolvimento deste trabalho e pensando em perspectivas futuras de pesquisas, foram vislumbrados alguns possíveis trabalhos. Entre as possibilidades encontram-se a continuidade deste trabalho no que tange o aumento da escalabilidade computacional, com o objetivo de verificar como esses modelos se comportam com a criação de uma estrutura em cluster ou grid computacional. Outra possibilidade está na expansão da análise realizada neste trabalho comparando a abordagem relacional tradicional e em memória, com alguma nova abordagem como o NewSQL, que mescla alta disponibilidade e escalabilidade com os requisitos transacionais e de consistência dos dados."
}