{
    "Autor": "Ronaldo Tadeu Murguero Junior",
    "Título": "Um Sistema de Manutenção Semiautomática de Ontologias a partir do Reconhecimento de Entidades",
    "Ano de publicação": 2013,
    "Local de publicação": "Araranguá",
    "Orientador(a)": "Professor Alexandre Leopoldo Gonçalves",
    "Coorientador(a)": "",
    "Resumo": "Uma quantidade cada vez maior de informações está disponível em formato textual e eletrônico. Essas informações contêm padrões textuais, tais como, conceitos, relacionamentos, regras, entre outros, podendo ser de grande auxílio na integração com outros sistemas ou mesmo, para auxiliar processos de tomada de decisão. Contudo, existe uma grande preocupação em como recuperar, organizar, armazenar e compartilhar estes padrões considerando uma formalização adequada. Neste sentido, a área de Extração de Informação promove suporte através de técnicas que analisam o texto e extraem padrões tidos como relevantes. Após a fase de extração, torna-se necessária a correta atribuição dos padrões para classes de um domínio em particular, em que estes passam a se chamar entidades. Tal processo é realizado através da subárea chamada de Reconhecimento de Entidades. Além disso, visando o compartilhamento e a manutenção de determinado domínio de conhecimento, as entidades devem ser armazenadas em um meio que possibilite atingir tais objetivos. Neste contexto a área de Ontologia se insere. Para demonstrar a viabilidade da proposição deste trabalho foi desenvolvido um protótipo voltado às fases de extração e reconhecimento de entidades, bem como a adição dessas entidades em uma ontologia para posterior manutenção. O processo de manutenção envolve a participação de um especialista de domínio responsável por validar os conceitos e modificar estes para as suas devidas classes quando necessário. Sendo assim, a manutenção pode ser entendida como semiautomática. De modo geral, a aplicação do protótipo em alguns cenários permitiu demonstrar que o sistema proposto é capaz de obter resultados satisfatórios, ainda que iniciais, mesmo que não exista conhecimento prévio de determinado domínio.",
    "Palavras-chave": [
        "Extração de Informação",
        "Reconhecimento de Entidades Nomeadas",
        "Ontologia",
        "Manutenção de Ontologia"
    ],
    "Introdução": "Uma quantidade cada vez maior dos dados existentes estão disponíveis na forma textual e eletrônica. Esses arquivos textuais contêm informações e, possivelmente padrões, que se analisados corretamente podem auxiliar no processo de tomada de decisão. Estes arquivos que estão disponíveis em meio eletrônico encontram-se principalmente na World Wide Web, ou simplesmente WEB. A WEB muitas vezes confunde-se com a Internet, esta criada durante a Guerra Fria para possibilitar a troca de informação. Porém, a WEB pode ser vista como um ambiente com interfaces mais amigáveis e intuitivas para o acesso ao crescente repositório de documentos, possuindo um diferencial em relação à Internet que são os hipertextos (SOUZA; ALVARENGA, 2004). Segundo Branski (2004), hipertextos funcionam como pontes que ligam dois pontos; na informática, isto é chamado de link. Links têm a capacidade de ligar palavras ou frases de uma página web a outros recursos da internet, fazendo com que o usuário explore a WEB. Desde a sua criação, a WEB tem crescido, segundo alguns autores, a taxas exponenciais (LYMAN; VARIAN, 2003; HIMMA, 2007). Segundo o estudo de Lyman e Varian (2003), a WEB era constituída por cerca de 167 terabytes de páginas estáticas. Contudo, considerando páginas geradas em tempo real a partir dos dados em bases de dados, e-mails e mensagens instantâneas, este valor atingia mais de 532 terabytes. No período de 1986 a 2007, segundo Hilbert (2011), a taxa de computação cresceu a uma porcentagem anual de 58%. Estes dados começaram a mudar, não sendo somente dos modos citados anteriormente; estão agora também no novo paradigma da WEB, chamado de WEB 2.0. Este novo paradigma foi citado pela primeira vez por O’Reilly e MediaLive International, onde foi citado que a Web 2.0 era composta agora por ferramentas que possibilitavam aos próprios usuários produzirem conteúdo na WEB (O’REILLY, 2007). Neste sentido, redes sociais, wikis e blogs se tornaram meios amplamente utilizados para popular a WEB. De modo geral, a informação divide-se em três tipos: sendo, Estruturada, quando o texto segue determinados padrões, possui uma estrutura definida, estes padrões são formatos de escrita que a EI conhece facilitando a sua consulta; Semi estruturada, quando os textos possuem certo padrão, mas não é todo escrito desta maneira; e Não estruturado, também chamado de texto livre, são os textos lidos diariamente, seja ele em um site de notícias, em blogs ou até mesmo na Wikipédia®, ou seja, não seguem um padrão de escrita pré-definido (ÁLVAREZ, 2007). Considerando o volume de informação e a sua natureza, tarefas voltadas ao armazenamento, gerenciamento e disponibilização se tornam desafios, principalmente para organizações que desejem obter alguma vantagem competitiva. Com o afirma Ceci (2010), as informações, na sua maioria, encontram-se dispersas sem uma adequada formalização e estrutura, necessitando de processamento adequado para que seja possível extrair padrões, regras e tendências que possam prover alguma utilidade no processo de tomada de decisão. Uma das ferramentas utilizadas para enfrentar estes desafios é a Extração de Informação (EI), que se preocupa com a extração de elementos relevantes em uma coleção de documentos (GRISHMAN, 1997; MUSLEA, 1999; ZAMBENEDETTI, 2002). Um dos componentes chaves da extração de informação são os seus padrões de extração ou regras de extração.",
    "Conclusão": "O objetivo geral desse trabalho foi desenvolver um sistema que permitisse a extração de entidades e a construção e a manutenção semiautomática de uma ontologia. Neste sentido, foi realizada uma revisão das áreas de extração de informações e ontologia visando dar suporte ao desenvolvimento do trabalho. A Extração de Informação constitui-se em uma subárea da área de Processamento de Linguagem Natural, que é amplamente utilizada para reconhecimento de padrões, aplicações de estatística e métodos de aprendizagem. A partir do estudo da área, este trabalho propôs uma abordagem estatística simplificada, conforme detalhamento apresentado no Capítulo 4. A extração de termos relevantes constitui-se no primeiro passo para o processo. O passo seguinte envolve a nomeação dos termos atribuindo a cada um deles para determinada classe. Nesse sentido, o Reconhecimento de Entidades Nomeadas (NER) é responsável pela identificação e classificação de entidades em textos, ou seja, o preenchimento de classes como, Pessoa e Organização. A partir do resultado prévio do processo de NER, torna-se necessário o armazenamento em algum meio físico, por exemplo, um banco de dados. Neste trabalho, utilizou-se uma Ontologia, pois esta permite representar vários conceitos de representação de conhecimento que não estão disponíveis em estruturas de dados tradicionais. Uma Ontologia permite representar conceitos abstratos de determinado domínio tais como Classes, Indivíduos e os relacionamentos entre eles que podem ser afirmados ou inferidos. Além disso, a ontologia permite o seu reuso, ou seja, a utilização de uma mesma ontologia em diferentes contextos de aplicação, visando à disseminação do conhecimento de um domínio. Para validar os conceitos envolvidos nesse trabalho, foi desenvolvido um protótipo voltado à extração de entidades e manutenção semiautomática de Ontologias. Ainda que a precisão do processo de extração seja baixa, quando comparado às entidades esperadas e as que foram corretamente identificadas, pode-se afirmar que o protótipo atendeu as expectativas, pois possibilita a extração de entidades de maneira rápida sem qualquer conhecimento prévio, ou seja, não existe a necessidade de uma fase de treinamento. Além disso, pode ser aplicado a textos pequenos, uma vez que a relevância estatística não é obtida considerando-se a palavra como um todo, mas sim pequenos tokens chamados de n-gramas. Outro ponto importante a mencionar é a diferença entre a proposição de visão lógica e física em relação à implementação do protótipo. Por questões de tempo, alguns passos importantes não foram realizados, por exemplo, a obtenção das tabelas léxicas (conjunto de palavras que definem determinada classe) não vêm da ontologia. Estas tabelas foram configuradas e definidas manualmente no código fonte do protótipo. Menciona-se ainda a não implementação de um analisador de expressões regulares, o que, em muitos casos, aumenta a precisão do reconhecimento de entidades. Ao longo do projeto foram vislumbradas novas possibilidades de melhorias para o projeto que não puderam ser desenvolvidas. Entre as possibilidades destacam-se a implementação de um analisador de expressões regulares e a criação de uma estrutura de processamento distribuída de modo que fosse possível a separação entre os processos de extração e classificação de termos. Outra possibilidade envolve a melhoria no algoritmo de reconhecimento de entidades, por exemplo, a implementação de um algoritmo de duas fases, em que a primeira fase analisa e computa os n-gramas relevantes para, em um segundo momento, realizar o reconhecimento. Deste modo, n-gramas que não são relevantes em um primeiro momento teriam sua importância alterada ao fim do processo, considerando toda a coleção, e deste modo, poderiam melhorar o reconhecimento de termos. Finalmente, mas sem exaurir as possibilidades, o processo de NER poderia ser aperfeiçoado se, de maneira incremental, o conhecimento armazenado na ontologia fosse utilizado cada vez que este fosse executado."
}