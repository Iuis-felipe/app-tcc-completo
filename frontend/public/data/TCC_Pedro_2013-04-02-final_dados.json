{
    "Autor": "Pedro Henrique Souza",
    "Título": "Um Sistema de Coleta de Dados de Fontes Heterogêneas Baseado em Computação Distribuída",
    "Ano de publicação": 2013,
    "Local de publicação": "Araranguá",
    "Orientador(a)": "Professor Alexandre Leopoldo Gonçalves",
    "Coorientador(a)": null,
    "Resumo": "Atualmente a quantidade de informação cresce de maneira exponencial, seja na Web ou redes internas das organizações. Um dos fatores que justificam esse fato é a participação cada vez mais frequente de usuários comuns não somente no consumo da informação, mas também na produção de conteúdo. Sendo assim, são requeridas maneiras eficientes de se coletar e armazenar grandes volumes de informação. Como tarefa, a coleta de informação constitui-se primeiramente na localização de determinada fonte de informação e posteriormente em sua coleta. De modo geral, as informações estão dispersas em servidores distribuídos geograficamente quando se fala na Web, mas também dentro das organizações espalhadas nos servidores e computadores pessoais. Coletar essa quantidade de informação exige poder de processamento computacional. Visando promover suporte a esta demanda o presente trabalho propõe um sistema em que a tarefa de coleta de dados seja realizada de maneira distribuída. A demonstração de viabilidade é realizada através de um protótipo implementado a partir da proposição de uma visão lógica (funcionamento geral) e física (detalhamento dos componentes tecnológicos). O protótipo desenvolvido contém os serviços de análise da estrutura de determinado recurso da Web e a coleta propriamente dita de maneira distribuída, permitindo deste modo, atender a demanda de um sistema de coleta de informação em grande escala. Visando possibilitar a análise do sistema proposto, são elaborados três cenários para verificar a adaptação do coletor, bem como, a sua capacidade de processamento. A aplicação do protótipo nestes cenários de coleta permitiu demonstrar que este é capaz de obter resultados consistentes e satisfatórios em relação à adaptação e desempenho em diferentes configurações, tanto na fase de análise quanto na fase de coleta.",
    "Palavras-chave": "Coleta de Dados; Web; Computação Distribuída; Coletor Distribuído.",
    "Introdução": "O aumento crescente da informação, em geral textos, produzida e publicada na web e nas organizações vem promovendo um incremento nas pesquisas de áreas capazes de lidar com a coleta, armazenamento, integração e a disponibilização dessa informação. Segundo autores como Greengrass (2000), Kobayashi e Takeda(2000), Lyman (2000) e Himma (2007), esse aumento ocorre de maneira exponencial, gerando assim desafios, principalmente no que tange a capacidade de coletar e processar informações de maneira eficiente. A pesquisa realizada por (LYMAN, 2000), apontou que a quantidade de conteúdos disponíveis na Internet duplicava anualmente, e estimou em mais de dois bilhões o número páginas disponíveis na Internet no início do ano 2000. Milhões de usuários publicam e têm acesso à informação livremente, fazendo uso da Internet para diversos objetivos (MODESTO et. al, 2005). Como exemplo, pode-se citar como atividades comuns dos usuários os uploads de vídeos ou fotos, atualização de seus status nas redes sociais, e o download dos mais variados arquivos. Pelo fato de ter um acesso fácil e rápido a todas as informações disponíveis, e todo usuário ser uma fonte de disseminação em potencial, a Internet vem se consolidando como o melhor e o mais eficiente meio de comunicação. Além do conteúdo disponível na Web, as organizações em geral produzem cada vez informações textuais. Segundo (BOVO, 2011), é possível localizar dentro das organizações, vários outros tipos de informação textual em formato digital, tais como: os diversos tipos de relatórios técnicos; manuais disponíveis sobre procedimentos ou softwares; pesquisas respondidas pelos usuários sobre satisfação de um produto ou serviço; os registros (arquivos de log) de sistemas de busca cooperativa ou mesmo de motores de busca de uso geral (search engines), como o Google ®; e-mails, currículos, e-books, mensagens de comunicação instantânea. Tal acúmulo de informação promove desafios para a coleta e disponibilização de modo que esta possa ser útil aos usuários que dela necessitam. O processo de coleta consiste basicamente na localização de conteúdo em fontes de informação, por exemplo, sites da Web, com o armazenamento em alguma base de dados para posterior disponibilização. Uma vez que se localize uma fonte de informação (um site qualquer) é necessário, a partir de um ponto de entrada (página principal, por exemplo), extrair os dados e as ligações com outras páginas internas ou externas à fonte de informação em particular que está sendo analisada. A coleta de dados é tratada na literatura como web crawler ou crawling, consistindo em um processo computacional que automatiza a recuperação de dados, nos mais variados formatos alcançados por uma variedade de protocolos (CHO, GARCIA-MOLINA; PAGE, 1998; GOMES; SILVA, 2008). Os dados em si são representados por arquivos, e os protocolos indicam quais fontes de informação podem ser inspecionadas. Um protocolo comum é o HTTP que suporta grande parte dos arquivos residentes na Web. Outro exemplo é o protocolo SMB que possibilita a coleta de arquivos que sejam compartilhados entre computadores utilizando sistemas operacionais como Linux™ e Windows™ (SANTOS, 2009). Contudo, para que os coletores atendam à demanda crescente por informações, diversas políticas devem ser seguidas. Isto se justifica uma vez que este processo demanda muita largura de banda, podendo ocasionar lentidão aos usuários que se utilizam da Web ou de redes internas nas organizações, e mesmo sobrecarregar os servidores com um número elevado de requisições. Deste modo, a eficiência é fundamental de modo que não prejudique o funcionamento de outros serviços da Web (CASTILLO, 2004). Como já mencionado, crawlers (coletores) se utilizam em geral dos links que constam nas páginas Web para acessar e coletar novas páginas. Entretanto, nem todas as informações estão acessíveis diretamente por links, precisando desta maneira, de uma resposta a consultas efetuadas a um banco de dados (PANAGIOTIS, 2001). A Web que não pode ser diretamente acessada por um coletor é chamada de Deep Web. Segundo pesquisa realizada por He (2007), a Deep Web permanece largamente inexplorada e com pouca cobertura pelos motores de busca, uma vez que seu conteúdo é gerado dinamicamente por sites. Esta fraca cobertura dos seus dados por motores de busca sugere que esta estrutura não é devidamente suportada. O autor ainda faz uma comparação com a superfície da Web que é grande, possui um rápido e diversificado crescimento, enquanto que na Deep Web, este crescimento é mais estruturado e sofre uma inerente limitação de rastreamento. Além da Web e Deep Web menciona-se ainda o fato de cada vez mais dados estarem sendo gerados pelos usuários. Estes passam de consumidores de informações estáticas conectadas por hiperlinks, a produtores de informações que geram impactos, tanto na disseminação da informação quanto nos mais variados modelos de negócio. Esta evolução vem sendo chamada de Web 2.0 (MODESTO et. al, 2005), uma vez que as informações passam a ser geradas a partir de um processo de socialização.",
    "Conclusão": "O objetivo geral desse trabalho foi o desenvolvimento de um sistema considerando uma arquitetura distribuída para a coleta de dados, capaz de atender diferentes cenários de coleta. Este objetivo foi possível através da pesquisa bibliográfica nas áreas relacionadas ao trabalho, coleta de dados e computação distribuída, bem como, pelo estudo de coletores existentes e frameworks que possibilitassem a distribuição do processo de coleta. Utilizando os conceitos obtidos por meio da revisão bibliográfica, foi proposto um sistema de coleta de informação com uma arquitetura distribuída. Para melhor compreensão do sistema proposto o mesmo foi dividido em duas partes representando a visão lógica e visão física. A visão lógica promove uma explicação das funcionalidades gerais do sistema envolvendo o próprio processo de coleta de dados, a obtenção de novos URLs, a análise de URLs e a distribuição do processamento. Com a visão física definiu-se o funcionamento dos componentes do sistema, explicando todos os passos, desde a adição de URLs (sementes), divisão destas pelo sistema, passando pela coleta das informações/dados e por fim, preservando-os em meio físico. Tais informações podem posteriormente ser indexadas e disponibilizadas através de um sistema de recuperação de informação. O protótipo desenvolvido atendeu as expectativas gerando resultados satisfatórios quando comparada a utilização de um único computador frente a utilização de um cluster. Os cenários propostos para a utilização do coletor permitiram analisar o comportamento do mesmo considerando diferentes demandas de coleta. De modo geral, durante a fase de teste do protótipo nos diferentes cenários notou-se que a falta de uma estratégica adequada no balanceamento de carga para a distribuição das tarefas entre os nodos do cluster afetou o desempenho final do sistema. Durante as análises foi possível verificar ainda que existe um número limite de nós a serem utilizados no cluster. O incremento de mais nós irá, em um determinado ponto, produzir resultados piores em relação a utilização de um único computador que possua as mesmas configurações dos nós. No decorrer do desenvolvimento do protótipo, houve a percepção de melhorias que poderiam ocorrer no sistema. Contudo, o desenvolvimento dessas melhorias tornaria o trabalho extenso. Entre as melhorias pode-se destacar a flexibilização da arquitetura, dividindo o protótipo em módulos de serviços. Cada parte do coletor poderia ser disponibilizada como um serviço web, como por exemplo, o preenchimento da lista de URLs (frontier) e a divisão do trabalho de coleta dos URLs. Outro ponto importante a ser melhorado no protótipo seria o método de divisão da tarefa de coleta entre os nós que participam do processamento. No estudo realizado neste trabalho os cenários tiveram estratégias diferentes. Neste sentido, seria adequado permitir que o método de divisão dos jobs fosse definido de maneira dinâmica, podendo ser inclusive disponibilizado na forma de um serviço web. Considerando a diversidade de assuntos, torna-se relevante que os coletores tenham a capacidade de analisar a semântica do conteúdo, visando deste modo um resultado mais qualificado considerando determinado domínio do interesse. Exemplo disto é o coletor proposto por Wang, Zhu e Li (2013), que consiste em coletar os comentários referentes à determinados produtos no site da Amazon™, e após a coleta, é realizada uma classificação do produto indicando se este teve ou não a aprovação de seus compradores. Por último, mas sem exaurir as possibilidades, a coleta focada usando ontologias vem se tornando a base de muitos estudos para um aprimoramento dos coletores. Como exemplo, podem-se citar as pesquisas de Zheng, Kang e Kim (2008), Jannach, Shchekotykhin e Friedrich (2009), Yang (2010) e Du e Hai (2013)."
}